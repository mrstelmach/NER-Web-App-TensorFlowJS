{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd0d1332-ffcc-4887-afd2-53aef2aa5573",
   "metadata": {},
   "source": [
    "# **Named Entity Recognition model on CoNLL 2003 dataset with Web App deployment**\n",
    "In this project we will employ deep learning techniques for developing a **Named Entity Recognition** (NER) model on CoNLL 2003 dataset and further deploy it as a web app. In a nutshell, NER is considered a task of searching and tagging important objects (i.e. named entities) like locations, people or organizations within a chunk of chosen text. From NLP and computational perspective NER is recognized as a sequence labelling problem â€“ given a text input, an algorithm should be capable of assigning aforementioned entity types to particular elements (tokens) of that sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc4de00-6b72-424e-a61e-abb6f822e54f",
   "metadata": {},
   "source": [
    "Project will be mainly conducted in **Python**, **Tensorflow** with **Keras**, with an inclusion of **Tensorflow.js** and **JavaScript** (required for web model deployment) and simple **html** script. Theoretical steps might be summarized by the following workflow:\n",
    "1. Explore data, extract inputs and corresponding tags (labels) from provided CoNLL 2003 txt files.\n",
    "2. Conduct any required text preprocessing, if necessary.\n",
    "3. Tokenize and create vocabulary.\n",
    "4. Text vectorization (representing texts in numerical form) of inputs and tags.\n",
    "5. Design LSTM based model architecture with a CNN extension.\n",
    "6. Train model with hyperparameter tuning.\n",
    "7. Evaluate performance on test data to compare against benchmarks.\n",
    "8. Design simple html website powered by Tensorflow.js converted Keras model.\n",
    "9. Website tests and final deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5912d15a-b4c9-4fe2-92e3-657d397f977c",
   "metadata": {},
   "source": [
    "#### **Import relevant libraries, functions, classes and initial config file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "097780ad-ea8c-4afc-ba75-1107c5fb8743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from evaluation.conlleval import evaluate\n",
    "from model_tf.model_tuner import tf_set_memory_growth\n",
    "\n",
    "from utils.encoders import SequentialLabelEncoder\n",
    "from utils.inference import getPredictedNER\n",
    "from utils.text import (CustomTokenizer, TextPreprocessor, read_txt_file,\n",
    "                        write_js_from_dict)\n",
    "\n",
    "with open('config.json') as config_file:\n",
    "    conf = json.load(config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da4f0c2-78b7-491b-b660-a613a2120457",
   "metadata": {},
   "source": [
    "Config file contains paths to relevant datasets and two variables defining maximum sequence length and maximum word length for model input. The last two variables are set to rather standard lengths so that most sentences and words do fit within specified ranges (otherwise they will be truncated). Config will be further extended by vocabulary sizes and number of entity tag classes (as those are required for building model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46f68bc0-868f-4165-9ce7-b27860ffef55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"PATH_TRAIN\": \"data/conll2003/train.txt\",\n",
      "  \"PATH_VALID\": \"data/conll2003/valid.txt\",\n",
      "  \"PATH_TEST\": \"data/conll2003/test.txt\",\n",
      "  \"MAX_SEQ_LEN\": 64,\n",
      "  \"MAX_WRD_LEN\": 16\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(conf, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9afed83-0426-4b7f-a471-6bace08b2b7a",
   "metadata": {},
   "source": [
    "#### **CoNLL 2003 dataset**\n",
    "An English version of CoNLL 2003 dataset will be used. The dataset consists of Reuters news stories gathered between 1996 and 1997 and is widely exploited across many NER researchers to produce state of the art results. The dataset is divided into train, validation and test files with a total of over 22k annotated sentences. Each file contains four single space separated columns with the first column being a word (token) of a given sentence (sentences are encoded in a row-wise manner) and the last one representing an annotated entity, i.e. label to predict for a particular word. Second and third ones are part-of-speech and syntactic tags respectively and are of our less interest in the NER context, at least for this particular project. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec46f26-e31a-49bd-b376-58c9f63fad21",
   "metadata": {},
   "source": [
    "Entities are split into four groups: person (B-PER, I-PER), location (B-LOC, I-LOC), organization (B-ORG, I- ORG) and miscellaneous (B-MISC, I-MISC). There is also a separate label O for no entity types. The I- prefix is applied in case of multi word entities of the same type to represent words following the first one. For example [European, Commission] would be tagged as [B-ORG, I-ORG]. More details on this dataset can be found here: https://www.clips.uantwerpen.be/conll2003/ner/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d280e4cd-e163-48a4-996e-42c67f39fd47",
   "metadata": {},
   "source": [
    "We can print out a few sentences from **train.txt** file for a more thorough inspection. Valid and test files are organized identically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcc45d9c-fb03-4f0d-9b27-567afa2dddc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-DOCSTART- -X- -X- O\n",
      "\n",
      "EU NNP B-NP B-ORG\n",
      "rejects VBZ B-VP O\n",
      "German JJ B-NP B-MISC\n",
      "call NN I-NP O\n",
      "to TO B-VP O\n",
      "boycott VB I-VP O\n",
      "British JJ B-NP B-MISC\n",
      "lamb NN I-NP O\n",
      ". . O O\n",
      "\n",
      "Peter NNP B-NP B-PER\n",
      "Blackburn NNP I-NP I-PER\n",
      "\n",
      "BRUSSELS NNP B-NP B-LOC\n",
      "1996-08-22 CD I-NP O\n",
      "\n",
      "The DT B-NP O\n",
      "European NNP I-NP B-ORG\n",
      "Commission NNP I-NP I-ORG\n",
      "said VBD B-VP O\n",
      "on IN B-PP O\n",
      "Thursday NNP B-NP O\n",
      "it PRP B-NP O\n",
      "disagreed VBD B-VP O\n",
      "with IN B-PP O\n",
      "German JJ B-NP B-MISC\n",
      "advice NN I-NP O\n",
      "to TO B-PP O\n",
      "consumers NNS B-NP O\n",
      "to TO B-VP O\n",
      "shun VB I-VP O\n",
      "British JJ B-NP B-MISC\n",
      "lamb NN I-NP O\n",
      "until IN B-SBAR O\n",
      "scientists NNS B-NP O\n",
      "determine VBP B-VP O\n",
      "whether IN B-SBAR O\n",
      "mad JJ B-NP O\n",
      "cow NN I-NP O\n",
      "disease NN I-NP O\n",
      "can MD B-VP O\n",
      "be VB I-VP O\n",
      "transmitted VBN I-VP O\n",
      "to TO B-PP O\n",
      "sheep NN B-NP O\n",
      ". . O O\n"
     ]
    }
   ],
   "source": [
    "with open(conf['PATH_TRAIN']) as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i < 48:\n",
    "            print(line.rstrip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6633a9-71d9-4a02-b445-6dfeac5d81b4",
   "metadata": {},
   "source": [
    "#### **Load data and preprocess**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49532ec8-5178-47bb-a38e-18bb67ed962b",
   "metadata": {},
   "source": [
    "To load data for further processing we will use a custom function `read_txt_file` available in `utils.text` module. This function simply returns two nested lists with the first one gathering sentences tokens and the latter representing corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8fe2409-9b2f-4c84-acb3-4a29c9ad1b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens and labels for the first sentence from training file: \n",
      " ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], \n",
      " ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O'] \n",
      "\n",
      "Number of train, valid and test instances are 14041, 3250 and 3453 respectively.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences_train, labels_train = read_txt_file(conf['PATH_TRAIN'])\n",
    "sentences_valid, labels_valid = read_txt_file(conf['PATH_VALID'])\n",
    "sentences_test, labels_test = read_txt_file(conf['PATH_TEST'])\n",
    "print('Tokens and labels for the first sentence from training file: \\n {}, \\n {} \\n'.format(\n",
    "    sentences_train[0], labels_train[0]))\n",
    "print('Number of train, valid and test instances are {}, {} and {} respectively.\\n'.format(\n",
    "    len(sentences_train), len(sentences_valid), len(sentences_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5de0c35-6c14-4516-a513-907e5b386748",
   "metadata": {},
   "source": [
    "As previously mentioned, sentences are already tokenized but to preprocess data and fit a tokenizer to create a generalizable algorithm we will need to proceed on standard, joined texts (this also mimics a production like behaviour as an end user will type in a text, not separate tokens). Thus, `texts` variable will be created on training data to further obtain vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41bb9dad-d7ab-4cd5-be8c-0f513dd27821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EU rejects German call to boycott British lamb .\n"
     ]
    }
   ],
   "source": [
    "texts = [' '.join(s) for s in sentences_train]\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1e4f8e-1e4a-448f-a6de-720410791ba5",
   "metadata": {},
   "source": [
    "Before tokenization we will preprocess texts with `TextPreprocessor` class from `utils.text`. This class separates apostrophes and punctuation from words as we would like to ensure that *John's* will not be treated as a different token than *John*. This is also in line with CoNLL 2003 dataset convention. For this project we will not remove any punctuation to preserve the full sequential sentence structure for the model input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "610711ad-4e90-4c0b-a228-52cb87febaa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 'Jack's son lives in London. I don't like him.' \n",
      "and after: 'Jack 's son lives in London . I do n't like him .' \n",
      "applying TextPreprocessor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_prc = TextPreprocessor(separate_apostrophes=True, separate_punctuation=True)\n",
    "texts = text_prc(texts)\n",
    "sample_text = \"Jack's son lives in London. I don't like him.\"\n",
    "print(\"Before: '{}' \\nand after: '{}' \\napplying TextPreprocessor.\\n\".format(sample_text, text_prc(sample_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ed45c3-80fc-4c01-83f1-aed7e1e3f71d",
   "metadata": {},
   "source": [
    "#### **Tokenization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aabc9e8-6e12-40a9-be85-bcb38ab44ca8",
   "metadata": {},
   "source": [
    "In this step we will define a `CustomTokenizer` class (based on `tf.keras.preprocessing.text.Tokenizer`) available in `utils.text`. This class provides a few important utilities for creating a proper numerical model input:\n",
    "1. Tokenize input on word level and apply lower case letters.\n",
    "2. Tokenize each word token on character level but without lower case letters.\n",
    "3. Add [UNK] and [PAD] (unknown and padding) tokens to vocabulary.\n",
    "4. Return word and character level token ids with additional mask indicating padded sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7f6751-f782-487a-9bb5-4504bd4fbbfe",
   "metadata": {},
   "source": [
    "Furthermore, `CustomTokenizer` class might be applied to pure text input (e.g. during inference), list of texts or list of lists of texts (the latter is a CoNLL 2003 case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f344ce9-03f9-47de-912c-129674dbfcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CustomTokenizer(char_level=True, lower=True)\n",
    "tokenizer.fit(texts, max_seq_len=conf['MAX_SEQ_LEN'], max_word_len=conf['MAX_WRD_LEN'])\n",
    "X_train = tokenizer.transform(sentences_train)\n",
    "X_valid = tokenizer.transform(sentences_valid)\n",
    "X_test = tokenizer.transform(sentences_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a06e2bc-db34-4f36-9af1-da8816f690bc",
   "metadata": {},
   "source": [
    "We can then inspect for example `X_train` object and its first instance to verify the result from applying tokenizer's `transform` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38d7e7ad-8d1f-4185-a057-3df67156a8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word tokens: \n",
      "['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\n",
      "\n",
      "Token ids on word level input: \n",
      "[  959 10390   216   645     8  3966   226  5756     2     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0]\n",
      "\n",
      "Token ids on character level input (for first 10 word tokens): \n",
      "[[38 60  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 9  3 64  3 14  5 10  0  0  0  0  0  0  0  0  0]\n",
      " [54  3  9 16  4  6  0  0  0  0  0  0  0  0  0  0]\n",
      " [14  4 11 11  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 5  8  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [23  8 21 14  8  5  5  0  0  0  0  0  0  0  0  0]\n",
      " [46  9  7  5  7 10 13  0  0  0  0  0  0  0  0  0]\n",
      " [11  4 16 23  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
      "\n",
      "Input mask: \n",
      "[1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Word tokens: \\n{}\\n'.format(sentences_train[0]))\n",
    "print('Token ids on word level input: \\n{}\\n'.format(X_train[0][0]))\n",
    "print('Token ids on character level input (for first 10 word tokens): \\n{}\\n'.format(X_train[1][0][:10]))\n",
    "print('Input mask: \\n{}\\n'.format(X_train[2][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a45b6e-aecc-44de-bb38-8371e5171da1",
   "metadata": {},
   "source": [
    "A few important things arise from matrices provided above. First off, both word and mask inputs are **2D** matrices while character level input is represented by a **3D** matrix - each sentence is a sequence of **MAX_SEQ_LEN** words (second dimension) and each word is a sequence of **MAX_WRD_LEN** characters (third dimension). To verify that we will provide all shapes in the cell below. Secondly, input mask has **1** in the position of any valid token (i.e. token that will be considered during model training or evaluation) and **0** otherwise (for padded tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5426b119-997f-4a44-b113-c8f785adfa4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes for word level input, character level input and input mask are (14041, 64), (14041, 64, 16) and (14041, 64) respectively.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Shapes for word level input, character level input and input mask are {}, {} and {} respectively.\\n'.format(\n",
    "    X_train[0].shape, X_train[1].shape, X_train[2].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd7f808-3eaf-45a9-9335-82119f1ab0cf",
   "metadata": {},
   "source": [
    "#### **Encode labels (tags)**\n",
    "The second step of vectorizing text is label encoding, i.e. assigning ids to corresponding labels (e.g. assign **0** id to **B-LOC** tag). For this purpose we will utilise `SequentialLabelEncoder` class (from `utils.encoders`) which uses a `sklearn.preprocessing.LabelEncoder` underneath but has two extra features: it pads sequences to a given length and additionally returns a mask (similar to the one from tokenization section). Thus, we end up with an **y** array that has consistent shape across all inputs and serves as a model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f7b71a7-3202-4d18-bc3f-3e2b7dd44f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"0\": \"B-LOC\",\n",
      "  \"1\": \"B-MISC\",\n",
      "  \"2\": \"B-ORG\",\n",
      "  \"3\": \"B-PER\",\n",
      "  \"4\": \"I-LOC\",\n",
      "  \"5\": \"I-MISC\",\n",
      "  \"6\": \"I-ORG\",\n",
      "  \"7\": \"I-PER\",\n",
      "  \"8\": \"O\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "lb_enc = SequentialLabelEncoder()\n",
    "lb_enc.fit(labels_train, max_seq_len=conf['MAX_SEQ_LEN'])\n",
    "labels_dict = dict(enumerate(lb_enc.le.classes_))\n",
    "print(json.dumps(labels_dict, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8632241-6358-4cc8-885f-6623615d76df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ids of labels with corresponding mask:\n",
      "['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O'],\n",
      "[2 8 1 8 8 8 1 8 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0],\n",
      "[1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "y_train, y_train_mask = lb_enc.transform(labels_train)\n",
    "y_valid, y_valid_mask = lb_enc.transform(labels_valid)\n",
    "y_test, y_test_mask = lb_enc.transform(labels_test)\n",
    "print('Ids of labels with corresponding mask:\\n{},\\n{},\\n{}'.format(labels_train[0], y_train[0], y_train_mask[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bb03fa-edc6-4755-8671-c19ae3af689e",
   "metadata": {},
   "source": [
    "Note that padding value (**0**) is the same as one of labels ids but it is not an issue as we are using mask and **0** will be only considered in cases where it represents an entity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8b50b0-9710-4556-a209-25c715e9858b",
   "metadata": {},
   "source": [
    "#### **Save objects**\n",
    "After applying all steps described so far we are ready to save (or update) different objects that will be further used for model trainig, inference and web app deployment:\n",
    "1. Data for model development and evaluation.\n",
    "2. Text preprocessor, tokenizer and label encoder (for inference).\n",
    "3. Vocabulary and label dictionaries (those are saved with `write_js_from_dict` function as **js** files for web app deployment).\n",
    "4. Config file updated with vocabularies and labels lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70d93914-0474-40d4-86ec-31f85935fc25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/data.joblib']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {'train': (X_train, y_train, y_train_mask),\n",
    "        'valid': (X_valid, y_valid, y_valid_mask), \n",
    "        'test': (X_test, y_test, y_test_mask)}\n",
    "joblib.dump(data, 'data/data.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e41b63a-1a00-461b-ab60-e1457efea84a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['inference/label_encoder.joblib']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(text_prc, 'inference/text_preprocessor.joblib')\n",
    "joblib.dump(tokenizer, 'inference/tokenizer.joblib')\n",
    "joblib.dump(lb_enc.le, 'inference/label_encoder.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5917cee-409d-496c-9f17-a4aaf8036d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_js_from_dict('web-app/vocabs/wordVocab.js', \n",
    "                   tokenizer.word_tokenizer.word_index,\n",
    "                   const_name='wordVocab')\n",
    "write_js_from_dict('web-app/vocabs/charVocab.js', \n",
    "                   tokenizer.char_tokenizer.word_index,\n",
    "                   const_name='charVocab')\n",
    "write_js_from_dict('web-app/vocabs/labels.js', \n",
    "                   labels_dict,\n",
    "                   const_name='labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f1a4443-0c75-4442-987f-5a0c0108a9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"PATH_TRAIN\": \"data/conll2003/train.txt\",\n",
      "  \"PATH_VALID\": \"data/conll2003/valid.txt\",\n",
      "  \"PATH_TEST\": \"data/conll2003/test.txt\",\n",
      "  \"MAX_SEQ_LEN\": 64,\n",
      "  \"MAX_WRD_LEN\": 16,\n",
      "  \"WORD_VOCAB_SIZE\": 17723,\n",
      "  \"CHAR_VOCAB_SIZE\": 87,\n",
      "  \"NUM_CLASSES\": 9\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "conf['WORD_VOCAB_SIZE'] = len(tokenizer.word_tokenizer.word_index)\n",
    "conf['CHAR_VOCAB_SIZE'] = len(tokenizer.char_tokenizer.word_index)\n",
    "conf['NUM_CLASSES'] = len(lb_enc.le.classes_)\n",
    "print(json.dumps(conf, indent=2))\n",
    "\n",
    "with open('config.json', 'w') as config_file:\n",
    "    json.dump(conf, config_file, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad4b467-ea44-4a13-80a4-6a431c6711fb",
   "metadata": {},
   "source": [
    "#### <br>**Develop model**\n",
    "Main deep learning model will be developed in Tensorflow and will be comprised of a few components:\n",
    "1. **Embedding** layers with pretrained **GloVe** embeddings for words (100 dimensional, available under https://nlp.stanford.edu/projects/glove/) and trained from scratch for characters.\n",
    "2. **1D Convolutional** layer with global max pooling to extract character features on word token level.\n",
    "3. **Bidirectional LSTM** layer that will sequentially proccess both word and character features (concatenated).\n",
    "4. **Dense** layer (directly connected to LSTM output) with a **relu** activation and **dropout** applied.\n",
    "5. **Dense** layer with a **softmax** activation to output entity tags probabilities for each word token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd6647d-e87d-4512-86a4-4d49ba0c24f9",
   "metadata": {},
   "source": [
    "As we are dealing with a **sequence to sequence** problem, we need a separate output for each sequence element (i.e. word token). That is why **Time Distributed** layer will be utilised. The model architecture also incorporates **mask** level input to ignore padded sequences during backpropagation. **Sample weights** are introduced in model fit procedure (and evaluation as well) to mask artificial (padded) labels (therefore weighted loss will not account for 0 weighted instances)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59254d8f-e165-4f23-83a7-76f20ab64ee9",
   "metadata": {},
   "source": [
    "For hyperparameter tuning with `keras-tuner` an efficient **Bayesian Optimization** algorithm will be used with a validation **weighted categorical crossentropy** being monitored as a primary metric (a weighted version of loss is of our main concern as it doesn't take into account padded labels). We run **100 trials** of hp tuning with a maximum of **50 epochs** per trial (**early stopping** is introduced as well) and a fixed **batch size** of **64**. **Hyperparameters** to be tuned are: character embedding dimension, number of dense and lstm units, number of convolutional filters, convolutional kernel size, dropout and learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c435770c-86c2-4968-b3cf-27ada58e9b1a",
   "metadata": {},
   "source": [
    "Final model is saved in h5 and tfjs format (for web app deployment). The whole thing is run with a **GPU** support. Tuning script is available as `model_tuner.py` and might be inspected below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ad6c42c-6724-4298-a544-5240d498363f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m# -*- coding: utf-8 -*-\u001b[39;49;00m\n",
      "\n",
      "\u001b[33m\"\"\"Hyperparameter tuning.\"\"\"\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjoblib\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mkeras_tuner\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mkt\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflowjs\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtfjs\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mkeras\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mlayers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m (Bidirectional, Concatenate, Conv1D,\n",
      "                                     Dense, Dropout, Embedding, GlobalMaxPool1D, \n",
      "                                     Input, LSTM, TimeDistributed)\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mtext\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m get_glove_embedding_matrix\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtf_set_memory_growth\u001b[39;49;00m():\n",
      "    \u001b[33m\"\"\"Set memory growth option for GPU device.\"\"\"\u001b[39;49;00m\n",
      "    gpu_devices = tf.config.experimental.list_physical_devices(\u001b[33m'\u001b[39;49;00m\u001b[33mGPU\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(gpu_devices) > \u001b[34m0\u001b[39;49;00m:\n",
      "        tf.config.experimental.set_memory_growth(gpu_devices[\u001b[34m0\u001b[39;49;00m], \u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mbuild_model\u001b[39;49;00m(hp):\n",
      "    \u001b[33m\"\"\"Build Keras model with hyperparameters.\"\"\"\u001b[39;49;00m\n",
      "    \n",
      "    \u001b[37m# hyperparameters\u001b[39;49;00m\n",
      "    dns_units = hp.Int(\u001b[33m'\u001b[39;49;00m\u001b[33mdns_units\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, min_value=\u001b[34m160\u001b[39;49;00m, max_value=\u001b[34m256\u001b[39;49;00m, step=\u001b[34m4\u001b[39;49;00m)\n",
      "    lstm_units = hp.Int(\u001b[33m'\u001b[39;49;00m\u001b[33mlstm_units\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, min_value=\u001b[34m64\u001b[39;49;00m, max_value=\u001b[34m160\u001b[39;49;00m, step=\u001b[34m4\u001b[39;49;00m)\n",
      "    ch_emb_dim = hp.Int(\u001b[33m'\u001b[39;49;00m\u001b[33mch_emb_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, min_value=\u001b[34m8\u001b[39;49;00m, max_value=\u001b[34m32\u001b[39;49;00m, step=\u001b[34m2\u001b[39;49;00m)\n",
      "    conv_filters = hp.Int(\u001b[33m'\u001b[39;49;00m\u001b[33mconv_filters\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, min_value=\u001b[34m16\u001b[39;49;00m, max_value=\u001b[34m48\u001b[39;49;00m, step=\u001b[34m2\u001b[39;49;00m)\n",
      "    conv_kernel = hp.Choice(\u001b[33m'\u001b[39;49;00m\u001b[33mconv_kernel\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, values=[\u001b[34m2\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m, \u001b[34m4\u001b[39;49;00m, \u001b[34m5\u001b[39;49;00m])\n",
      "    drp = hp.Float(\u001b[33m'\u001b[39;49;00m\u001b[33mdrp\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, min_value=\u001b[34m0.2\u001b[39;49;00m, max_value=\u001b[34m0.5\u001b[39;49;00m, step=\u001b[34m0.025\u001b[39;49;00m)\n",
      "    lr = hp.Choice(\u001b[33m'\u001b[39;49;00m\u001b[33mlr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, values=[\u001b[34m5e-4\u001b[39;49;00m, \u001b[34m7.5e-4\u001b[39;49;00m, \u001b[34m1e-3\u001b[39;49;00m])\n",
      "\n",
      "    \u001b[37m# mask input on word sequence level\u001b[39;49;00m\n",
      "    mask_inp = Input(shape=(conf[\u001b[33m'\u001b[39;49;00m\u001b[33mMAX_SEQ_LEN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],), name=\u001b[33m\"\u001b[39;49;00m\u001b[33mmask_input\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[37m# word level input\u001b[39;49;00m\n",
      "    word_inp = Input(shape=(conf[\u001b[33m'\u001b[39;49;00m\u001b[33mMAX_SEQ_LEN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],), name=\u001b[33m'\u001b[39;49;00m\u001b[33mword_input\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    word_emb = Embedding(input_dim=conf[\u001b[33m'\u001b[39;49;00m\u001b[33mWORD_VOCAB_SIZE\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], \n",
      "                         output_dim=EMBEDDING_MATRIX.shape[\u001b[34m1\u001b[39;49;00m], mask_zero=\u001b[34mTrue\u001b[39;49;00m, \n",
      "                         input_length=conf[\u001b[33m'\u001b[39;49;00m\u001b[33mMAX_SEQ_LEN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\n",
      "                         weights=[EMBEDDING_MATRIX], trainable=\u001b[34mFalse\u001b[39;49;00m, \n",
      "                         name=\u001b[33m'\u001b[39;49;00m\u001b[33mword_embedding\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(word_inp)\n",
      "    \n",
      "    \u001b[37m# character level input\u001b[39;49;00m\n",
      "    char_inp = Input(shape=(conf[\u001b[33m'\u001b[39;49;00m\u001b[33mMAX_SEQ_LEN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], conf[\u001b[33m'\u001b[39;49;00m\u001b[33mMAX_WRD_LEN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]), \n",
      "                     name=\u001b[33m\"\u001b[39;49;00m\u001b[33mcharacter_input\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    char_emb = TimeDistributed(\n",
      "        Embedding(input_dim=conf[\u001b[33m'\u001b[39;49;00m\u001b[33mCHAR_VOCAB_SIZE\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], output_dim=ch_emb_dim, \n",
      "                  input_length=conf[\u001b[33m'\u001b[39;49;00m\u001b[33mMAX_WRD_LEN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]),\n",
      "        name=\u001b[33m\"\u001b[39;49;00m\u001b[33mcharacter_embedding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)(char_inp)\n",
      "    conv1d = TimeDistributed(\n",
      "        Conv1D(filters=conv_filters, kernel_size=conv_kernel, \n",
      "               strides=\u001b[34m1\u001b[39;49;00m, padding=\u001b[33m'\u001b[39;49;00m\u001b[33msame\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, activation=\u001b[33m'\u001b[39;49;00m\u001b[33mtanh\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), \n",
      "        name=\u001b[33m\"\u001b[39;49;00m\u001b[33m1d_character_convolution\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)(char_emb)\n",
      "    char_features = TimeDistributed(\n",
      "        GlobalMaxPool1D(), \n",
      "        name=\u001b[33m\"\u001b[39;49;00m\u001b[33mglobal_max_pooling\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)(conv1d)\n",
      "    \n",
      "    \u001b[37m# concatenation and output\u001b[39;49;00m\n",
      "    conc = Concatenate(name=\u001b[33m'\u001b[39;49;00m\u001b[33mconcatenation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)([word_emb, char_features])\n",
      "    lstm = Bidirectional(LSTM(lstm_units, return_sequences=\u001b[34mTrue\u001b[39;49;00m),\n",
      "                         name=\u001b[33m'\u001b[39;49;00m\u001b[33mbi_lstm\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(conc)\n",
      "    dns = TimeDistributed(\n",
      "        Dense(dns_units, activation=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), \n",
      "        name=\u001b[33m'\u001b[39;49;00m\u001b[33mdense\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(lstm)\n",
      "    dns_drop = Dropout(drp, name=\u001b[33m'\u001b[39;49;00m\u001b[33mdense_dropout\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(dns)\n",
      "    out = TimeDistributed(\n",
      "        Dense(conf[\u001b[33m'\u001b[39;49;00m\u001b[33mNUM_CLASSES\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], activation=\u001b[33m'\u001b[39;49;00m\u001b[33msoftmax\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), \n",
      "        name=\u001b[33m'\u001b[39;49;00m\u001b[33moutput\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(dns_drop, mask=mask_inp)\n",
      "    \n",
      "    model = tf.keras.Model([word_inp, char_inp, mask_inp], out)\n",
      "    model.compile(optimizer=tf.keras.optimizers.Adam(lr),\n",
      "                  loss=\u001b[33m'\u001b[39;49;00m\u001b[33msparse_categorical_crossentropy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                  weighted_metrics=[\u001b[33m'\u001b[39;49;00m\u001b[33msparse_categorical_crossentropy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                                    \u001b[33m'\u001b[39;49;00m\u001b[33msparse_categorical_accuracy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m model\n",
      "\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "    \n",
      "    \u001b[37m# load config file, development data and pretrained embeddings \u001b[39;49;00m\n",
      "    \u001b[37m# for building model and hyperparameter tuning with Bayesian Optimization; \u001b[39;49;00m\n",
      "    \u001b[37m# save best model in h5 format and convert to tfjs for web app deployment;\u001b[39;49;00m\n",
      "    \u001b[37m# evaluate on train, validation and test datasets;\u001b[39;49;00m\n",
      "    \n",
      "    tf_set_memory_growth()\n",
      "    \n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mconfig.json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m config_file:\n",
      "        conf = json.load(config_file)\n",
      "        \n",
      "    data = joblib.load(\u001b[33m'\u001b[39;49;00m\u001b[33mdata/data.joblib\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    X_train, y_train, y_train_mask = data[\u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "    X_valid, y_valid, y_valid_mask = data[\u001b[33m'\u001b[39;49;00m\u001b[33mvalid\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "    X_test, y_test, y_test_mask = data[\u001b[33m'\u001b[39;49;00m\u001b[33mtest\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "    \n",
      "    word_index = (joblib.load(\u001b[33m'\u001b[39;49;00m\u001b[33minference/tokenizer.joblib\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "                  .word_tokenizer.word_index)\n",
      "    EMBEDDING_MATRIX = get_glove_embedding_matrix(\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33mdata/glove.6B.100d.txt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, word_index\n",
      "    )\n",
      "    \n",
      "    es_callback = tf.keras.callbacks.EarlyStopping(\n",
      "        monitor=\u001b[33m'\u001b[39;49;00m\u001b[33mval_sparse_categorical_crossentropy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, min_delta=\u001b[34m0\u001b[39;49;00m, \n",
      "        patience=\u001b[34m2\u001b[39;49;00m, verbose=\u001b[34m0\u001b[39;49;00m, mode=\u001b[33m'\u001b[39;49;00m\u001b[33mmin\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, baseline=\u001b[34mNone\u001b[39;49;00m, \n",
      "        restore_best_weights=\u001b[34mTrue\u001b[39;49;00m\n",
      "        )\n",
      "\n",
      "    tuner = kt.BayesianOptimization(\n",
      "        hypermodel=build_model, objective=\u001b[33m'\u001b[39;49;00m\u001b[33mval_sparse_categorical_crossentropy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "        max_trials=\u001b[34m100\u001b[39;49;00m, num_initial_points=\u001b[34m10\u001b[39;49;00m, directory=\u001b[33m'\u001b[39;49;00m\u001b[33mmodel_tf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "        project_name=\u001b[33m'\u001b[39;49;00m\u001b[33mhptuning\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "        )\n",
      "    \n",
      "    fit_args = {\u001b[33m'\u001b[39;49;00m\u001b[33mx\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: X_train, \u001b[33m'\u001b[39;49;00m\u001b[33my\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: y_train, \u001b[33m'\u001b[39;49;00m\u001b[33msample_weight\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: y_train_mask, \n",
      "                \u001b[33m'\u001b[39;49;00m\u001b[33mvalidation_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: (X_valid, y_valid, y_valid_mask), \n",
      "                \u001b[33m'\u001b[39;49;00m\u001b[33mepochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[34m50\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[34m64\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mshuffle\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[34mTrue\u001b[39;49;00m,\n",
      "                \u001b[33m'\u001b[39;49;00m\u001b[33mverbose\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[34m2\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mcallbacks\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: [es_callback]}\n",
      "\n",
      "    tuner.search(**fit_args)\n",
      "    best_hparams = tuner.get_best_hyperparameters(num_trials=\u001b[34m1\u001b[39;49;00m)[\u001b[34m0\u001b[39;49;00m]\n",
      "    model = tuner.hypermodel.build(best_hparams)\n",
      "    \u001b[36mprint\u001b[39;49;00m(model.summary())\n",
      "    model.fit(**fit_args)\n",
      "    \n",
      "    model.save(\u001b[33m'\u001b[39;49;00m\u001b[33minference/h5_model/model.h5\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    tfjs.converters.save_keras_model(model, \u001b[33m'\u001b[39;49;00m\u001b[33mweb-app/tfjs_model\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mCategorical crossentropy and Accuracy for training data: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(\n",
      "        model.evaluate(x=X_train, y=y_train, sample_weight=y_train_mask, verbose=\u001b[34m0\u001b[39;49;00m)[\u001b[34m1\u001b[39;49;00m:]))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mCategorical crossentropy and Accuracy for validation data: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(\n",
      "        model.evaluate(x=X_valid, y=y_valid, sample_weight=y_valid_mask, verbose=\u001b[34m0\u001b[39;49;00m)[\u001b[34m1\u001b[39;49;00m:]))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mCategorical crossentropy and Accuracy for test data: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(\n",
      "        model.evaluate(x=X_test, y=y_test, sample_weight=y_test_mask, verbose=\u001b[34m0\u001b[39;49;00m)[\u001b[34m1\u001b[39;49;00m:]))\n"
     ]
    }
   ],
   "source": [
    "!pygmentize model_tf/model_tuner.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9493b5-18a2-4fda-af71-1a521cca7b17",
   "metadata": {},
   "source": [
    "<br>We will run the background script from bash and save the log for further examination. After the process is finished, train, validation and test datasets will be evaluated and metrics monitored during training will be printed out from log file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1681e20e-504b-4800-9931-70b9985519fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('nohup python -m model_tf.model_tuner > model_tf/model_tuner.log &')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1bd75298-ef11-47f1-8ab7-aa8c82bc924e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical crossentropy and Accuracy for training data: [0.04258184880018234, 0.9876053333282471].\n",
      "Categorical crossentropy and Accuracy for validation data: [0.07042563706636429, 0.9788177609443665].\n",
      "Categorical crossentropy and Accuracy for test data: [0.10563959181308746, 0.9671876430511475].\n"
     ]
    }
   ],
   "source": [
    "with open('model_tf/model_tuner.log') as log_file:\n",
    "    for line in log_file:\n",
    "        if 'Categorical crossentropy and Accuracy for' in line:\n",
    "            print(line.rstrip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05075241-1357-4c88-8064-8e193a5ff7b5",
   "metadata": {},
   "source": [
    "#### <br>**Evaluate with F1 score**\n",
    "Developed model will be additionally evaluated with **F1 score**, a typical metric for assessing NER systems, which is a weighted average of both recall and precision. Precision is the percentage of named entities found by the learning system that are correct while recall is the percentage of named entities present in the corpus that are found by the system. An evaluation standard is that we operate on entity level, therefore *[New, York] [B-LOC, I-LOC]* becomes one instance (*LOC*). An exact match only is considered a correct prediction therefore any partial matches where algorithm correctly identifies an entity existence but predicts the wrong type are discarded. Recall and precision metrics are computed globally by counting the total number of true positives, false negatives and false positives, therefore F1 metric is **micro-averaged**.<br> https://www.clips.uantwerpen.be/conll2000/chunking/output.html provides an official evaluation Perl script (`conlleval`) that will be used in our case and can be found in `evaluation` folder. For that purpose we need to prepare a proper input in a txt file with token, true label and predicted label in each line and sentences separated by empty lines. `create_eval_file` from `utils.text` does exactly that. This function is preceeded by `indices_to_labels` (from `utils.encoders`) to obtain token corresponding labels from model predicted ids. To automate those activities a wrapper Python function called `evaluate` (from `evaluation.conlleval`) will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "160615fb-7b42-4969-86f8-57f240dc63ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 46324 tokens with 5626 phrases; found: 5923 phrases; correct: 4682.\n",
      "accuracy:  96.72%; precision:  79.05%; recall:  83.22%; FB1:  81.08\n",
      "              LOC: precision:  80.92%; recall:  90.37%; FB1:  85.38  1855\n",
      "             MISC: precision:  58.03%; recall:  78.21%; FB1:  66.63  946\n",
      "              ORG: precision:  80.24%; recall:  71.64%; FB1:  75.70  1483\n",
      "              PER: precision:  87.98%; recall:  90.01%; FB1:  88.98  1639\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences_test, labels_test = read_txt_file(conf['PATH_TEST'])\n",
    "label_encoder = joblib.load('inference/label_encoder.joblib')\n",
    "X_test, _, y_test_mask = joblib.load('data/data.joblib')['test']\n",
    "\n",
    "tf_set_memory_growth()\n",
    "model = tf.keras.models.load_model('inference/h5_model/model.h5')\n",
    "\n",
    "evaluate(sentences_test, labels_test, label_encoder, X_test, y_test_mask, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52cfcd1-1bb5-418a-befc-fa9393eef4e9",
   "metadata": {},
   "source": [
    "Obtained F1 Score (**FB1: 81.08**) might be considered satisfactory as it is way above the baseline reported for CoNLL 2003 dataset (**59.61**, https://aclweb.org/aclwiki/CONLL-2003_(State_of_the_art)) and is only a few percentage points lower than other similar deep learning approaches available under https://paperswithcode.com/sota/named-entity-recognition-ner-on-conll-2003. An important thing to note is that the main idea of this project is not to push the state of the art results but rather to create a decently performing, moderately simple and generalizable algorithm that would be further deployed in a browser. To boost the score one may use a more complex architecture (for example transformer based), higher dimensional embeddings, lexicons, larger external vocabulary from pretrained models (e.g. BERT like) or pretrained models themselves. External vocabulary would be also very beneficial in terms of preventing overfitting as using the vocabulary only from the training data might naturally skew the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329b9fb3-798f-49e8-90ef-68883e6680d2",
   "metadata": {},
   "source": [
    "#### **Deploy Web App**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78c8f49-1f18-481f-ad0e-8ed78238f05c",
   "metadata": {},
   "source": [
    "All components required for developing and deploying web application powered by Tensorflow model are available in `web-app` folder. Those include in particular:\n",
    "1. `ner-web-app.html` and `style.css` files with the whole web design.\n",
    "2. `tfjs_model` folder with Tensorflow.js converted model object.\n",
    "3. `vocabs` folder with word/character vocabularies and dictionary with labels (all stored in js files).\n",
    "4. `javascript` folder with scripts for loading model, handling text input, getting predictions and specific text formatting for output. Functions for (pre)processing text (separating apostrophes, tokenization etc.) are similar in behaviour to those used in Python during model development."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90d5880-b39a-404a-9c4b-aef81a43f327",
   "metadata": {},
   "source": [
    "To ensure that deployed model works properly we will compare the browser output with a Python model in *pseudo* inference mode (*pseudo* as it only serves as an example and is not deployed actually). Therefore a simple `getPredictedNER` class that takes text as input and returns entity tags is introduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "320221a0-867f-4647-aebb-970036850e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m# -*- coding: utf-8 -*-\u001b[39;49;00m\n",
      "\n",
      "\u001b[33m\"\"\"Running model in inference mode.\"\"\"\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdataclasses\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m dataclass\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mkeras\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Model \u001b[34mas\u001b[39;49;00m KerasModel\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mtext\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m CustomTokenizer, TextPreprocessor\n",
      "\n",
      "\n",
      "\u001b[90m@dataclass\u001b[39;49;00m\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mgetPredictedNER\u001b[39;49;00m:\n",
      "    \u001b[33m\"\"\"End to end NER prediction for given text.\"\"\"\u001b[39;49;00m\n",
      "    preprocessor: TextPreprocessor\n",
      "    tokenizer: CustomTokenizer\n",
      "    model: KerasModel\n",
      "    labels: \u001b[36mlist\u001b[39;49;00m\n",
      "    \n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__call__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, input_text):\n",
      "        input_text = \u001b[36mself\u001b[39;49;00m.preprocessor(input_text)\n",
      "        input_X = \u001b[36mself\u001b[39;49;00m.tokenizer.transform(input_text)\n",
      "        input_mask = np.squeeze(input_X[-\u001b[34m1\u001b[39;49;00m]).astype(\u001b[36mbool\u001b[39;49;00m)\n",
      "        input_tokens = \u001b[36mself\u001b[39;49;00m.tokenizer.get_original_tokens(input_text)\n",
      "        pred_ids = np.argmax(\u001b[36mself\u001b[39;49;00m.model.predict(input_X), axis=-\u001b[34m1\u001b[39;49;00m)\n",
      "        pred_ids = np.squeeze(pred_ids)[input_mask]\n",
      "        pred_cls = [\u001b[36mself\u001b[39;49;00m.labels[idx] \u001b[34mfor\u001b[39;49;00m idx \u001b[35min\u001b[39;49;00m pred_ids]\n",
      "        ner_output = [(tkn, ner) \u001b[34mfor\u001b[39;49;00m tkn, ner \n",
      "                      \u001b[35min\u001b[39;49;00m \u001b[36mzip\u001b[39;49;00m(input_tokens, pred_cls)]\n",
      "        \n",
      "        \u001b[34mreturn\u001b[39;49;00m ner_output\n"
     ]
    }
   ],
   "source": [
    "!pygmentize utils/inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac50cb2-8ddc-4181-8ebc-43bccc894828",
   "metadata": {},
   "source": [
    "For that class, text preprocessor, tokenizer, Keras model and labels should be provided - all available in `inference` folder. We will also get two sample texts from different sources to be used in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5b221da-bb86-4a63-9689-a2e5387b4f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all required objects for inference are already available in the worksapce\n",
    "# though we still load them to check for completeness\n",
    "text_prc = joblib.load('inference/text_preprocessor.joblib')\n",
    "tokenizer = joblib.load('inference/tokenizer.joblib')\n",
    "tf_set_memory_growth()\n",
    "model = tf.keras.models.load_model('inference/h5_model/model.h5')\n",
    "label_encoder = joblib.load('inference/label_encoder.joblib')\n",
    "\n",
    "get_ner = getPredictedNER(text_prc, tokenizer, model, label_encoder.classes_.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "628c9503-9790-470c-8d52-795030d39dfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Abraham', 'B-PER'),\n",
       " ('Lincoln', 'I-PER'),\n",
       " ('(', 'O'),\n",
       " ('February', 'O'),\n",
       " ('12', 'O'),\n",
       " (',', 'O'),\n",
       " ('1809', 'O'),\n",
       " ('â€“', 'O'),\n",
       " ('April', 'O'),\n",
       " ('15', 'O'),\n",
       " (',', 'O'),\n",
       " ('1865', 'O'),\n",
       " (')', 'O'),\n",
       " ('was', 'O'),\n",
       " ('an', 'O'),\n",
       " ('American', 'B-MISC'),\n",
       " ('lawyer', 'O'),\n",
       " ('and', 'O'),\n",
       " ('statesman', 'O'),\n",
       " ('who', 'O'),\n",
       " ('served', 'O'),\n",
       " ('as', 'O'),\n",
       " ('the', 'O'),\n",
       " ('16th', 'O'),\n",
       " ('president', 'O'),\n",
       " ('of', 'O'),\n",
       " ('the', 'O'),\n",
       " ('United', 'B-LOC'),\n",
       " ('States', 'I-LOC'),\n",
       " ('from', 'O'),\n",
       " ('1861', 'O'),\n",
       " ('until', 'O'),\n",
       " ('his', 'O'),\n",
       " ('assassination', 'O'),\n",
       " ('in', 'O'),\n",
       " ('1865', 'O'),\n",
       " ('.', 'O'),\n",
       " ('Lincoln', 'B-LOC'),\n",
       " ('led', 'O'),\n",
       " ('the', 'O'),\n",
       " ('nation', 'O'),\n",
       " ('through', 'O'),\n",
       " ('the', 'O'),\n",
       " ('American', 'B-MISC'),\n",
       " ('Civil', 'I-MISC'),\n",
       " ('War', 'I-MISC'),\n",
       " (',', 'O'),\n",
       " ('the', 'O'),\n",
       " ('country', 'O'),\n",
       " (\"'s\", 'O'),\n",
       " ('greatest', 'O'),\n",
       " ('moral', 'O'),\n",
       " (',', 'O'),\n",
       " ('cultural', 'O'),\n",
       " (',', 'O'),\n",
       " ('constitutional', 'O'),\n",
       " (',', 'O'),\n",
       " ('and', 'O'),\n",
       " ('political', 'O'),\n",
       " ('crisis', 'O'),\n",
       " ('.', 'O')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt1 = \"Abraham Lincoln (February 12, 1809 â€“ April 15, 1865) was an American lawyer and statesman who served as the 16th president of the United States from 1861 until his assassination in 1865. Lincoln led the nation through the American Civil War, the country's greatest moral, cultural, constitutional, and political crisis.\"\n",
    "get_ner(txt1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a1deac-ea3c-4665-aee2-6f14ee98c7c4",
   "metadata": {},
   "source": [
    "And the corresponding outputs from web application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3655a10-f9dd-45c6-a026-07da24a10667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('London', 'B-LOC'),\n",
       " ('is', 'O'),\n",
       " ('the', 'O'),\n",
       " ('capital', 'O'),\n",
       " ('and', 'O'),\n",
       " ('largest', 'O'),\n",
       " ('city', 'O'),\n",
       " ('of', 'O'),\n",
       " ('England', 'B-LOC'),\n",
       " ('and', 'O'),\n",
       " ('the', 'O'),\n",
       " ('United', 'B-LOC'),\n",
       " ('Kingdom', 'I-LOC'),\n",
       " ('.', 'O'),\n",
       " ('The', 'O'),\n",
       " ('city', 'O'),\n",
       " ('stands', 'O'),\n",
       " ('on', 'O'),\n",
       " ('the', 'O'),\n",
       " ('River', 'B-LOC'),\n",
       " ('Thames', 'I-LOC'),\n",
       " ('in', 'O'),\n",
       " ('the', 'O'),\n",
       " ('south', 'O'),\n",
       " ('-', 'O'),\n",
       " ('east', 'O'),\n",
       " ('of', 'O'),\n",
       " ('England', 'B-LOC'),\n",
       " (',', 'O'),\n",
       " ('at', 'O'),\n",
       " ('the', 'O'),\n",
       " ('head', 'O'),\n",
       " ('of', 'O'),\n",
       " ('its', 'O'),\n",
       " ('50', 'O'),\n",
       " ('-', 'O'),\n",
       " ('mile', 'O'),\n",
       " ('(', 'O'),\n",
       " ('80', 'O'),\n",
       " ('km', 'O'),\n",
       " (')', 'O'),\n",
       " ('estuary', 'O'),\n",
       " ('leading', 'O'),\n",
       " ('to', 'O'),\n",
       " ('the', 'O'),\n",
       " ('North', 'B-LOC'),\n",
       " ('Sea', 'I-LOC'),\n",
       " ('.', 'O')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt2 = \"London is the capital and largest city of England and the United Kingdom. The city stands on the River Thames in the south-east of England, at the head of its 50-mile (80 km) estuary leading to the North Sea.\"\n",
    "get_ner(txt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5fcd00-6b03-433f-be1d-a4942b29ad62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-gpu] *",
   "language": "python",
   "name": "conda-env-tf-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
